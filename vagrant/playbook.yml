---
- hosts: all
  become: true
  
  tasks:
    - name: Install Docker
      apt:
        name: docker.io
        state: present
        update_cache: true

    - name: Check if Kind is installed
      stat:
        path: /usr/local/bin/kind
      register: kind_binary

    - name: Install Kind
      get_url:
        url: https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64
        dest: /usr/local/bin/kind
        mode: '0755'
      when:
        - ansible_facts['os_family'] == "Debian"
        - not kind_binary.stat.exists 

    - name: Check if kubectl is installed
      stat:
        path: /usr/local/bin/kubectl
      register: kubectl_binary         

    - name: Install kubectl
      get_url:
        url: https://storage.googleapis.com/kubernetes-release/release/v1.20.0/bin/linux/amd64/kubectl
        dest: /usr/local/bin/kubectl
        mode: '0755'
      when:
        - ansible_facts['os_family'] == "Debian"
        - not kubectl_binary.stat.exists

    - name: Check if Kind cluster exists
      command: kind get clusters
      register: kind_clusters
      changed_when: false

    - name: Create Kind config file
      template:
        src: kind-config.yaml.j2
        dest: /tmp/kind-config.yaml     
      when: inventory_hostname not in kind_clusters.stdout           

    - name: Create Kind cluster if it does not exist
      command: kind create cluster --name {{ inventory_hostname }} --config=/tmp/kind-config.yaml
      when: inventory_hostname not in kind_clusters.stdout

    - name: Check if ArgoCD namespace exists
      command: kubectl --context kind-{{ inventory_hostname }} get namespace argocd --ignore-not-found
      register: argocd_namespace
      changed_when: false

    - name: Install ArgoCD if not already installed
      shell: |
        kubectl --context kind-{{ inventory_hostname }} create namespace argocd
        kubectl --context kind-{{ inventory_hostname }} apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
      when: argocd_namespace.stdout == ""
      args:
        executable: /bin/bash

    - name: Expose ArgoCD server as NodePort if not already exposed
      shell: |
        kubectl --context kind-{{ inventory_hostname }} -n argocd patch svc argocd-server -p '{"spec": {"type": "NodePort", "ports": [{"port": 443, "targetPort": 8080, "nodePort": 32000}]}}'

      args:
        executable: /bin/bash
      when: argocd_namespace.stdout == ""     

    - name: Deploy root App of Apps in ArgoCD
      command: >
        kubectl --context kind-{{ inventory_hostname }} apply -f ./{{ inventory_hostname }}/root-app.yaml

    - name: Get kubeconfig for cluster
      command: kind get kubeconfig --name {{ inventory_hostname }}
      register: cluster_kubeconfig

    - name: Retrieve cluster API endpoint
      command: kubectl config view --raw --minify -o jsonpath='{.clusters[0].cluster.server}' --context kind-{{ inventory_hostname }}
      register: cluster_api_endpoint      

    - name: Base64 encode the kubeconfig for cluster
      shell: echo "{{ cluster_kubeconfig.stdout }}" | base64 | tr -d '\n'
      register: kubeconfig_b64      

    - name: Create Istio-compatible secret YAML
      copy:
        dest: ./{{ inventory_hostname }}/{{ inventory_hostname }}-cluster-secret.yaml
        content: |
          apiVersion: v1
          kind: Secret
          metadata:
            name: istio-remote-cluster
            namespace: istio-system
          data:
            # Base64-encoded kubeconfig for the cluster
            kubeconfig: "{{ kubeconfig_b64.stdout }}"   

    - name: Check if Helm is installed
      command: helm version --short
      register: helm_installed
      ignore_errors: true

    - name: Install Helm if not installed
      block:
        - name: Download Helm tarball
          get_url:
            url: https://get.helm.sh/helm-v3.11.2-linux-amd64.tar.gz  # Adjust to the latest version
            dest: /tmp/helm-v3.11.2-linux-amd64.tar.gz

        - name: Extract Helm tarball
          unarchive:
            src: /tmp/helm-v3.11.2-linux-amd64.tar.gz
            dest: /tmp/
            remote_src: yes

        - name: Move Helm binary to /usr/local/bin
          command: mv /tmp/linux-amd64/helm /usr/local/bin/helm
          become: true

        - name: Verify Helm installation
          command: helm version --short
          register: helm_version
          become: true

      when: helm_installed.rc != 0  # Only run this block if Helm is not installed   

    - name: Add HashiCorp Helm repository
      command: helm repo add hashicorp https://helm.releases.hashicorp.com

    - name: Update Helm repositories
      command: helm repo update     

    - name: Create the Vault namespace
      command: kubectl create namespace vault --context kind-{{ inventory_hostname }}
      ignore_errors: true  # Ignore error if namespace already exists    

    - name: Check if Vault pod is already running
      command: kubectl --context kind-{{ inventory_hostname }} get pods vault-0 -n vault --no-headers
      register: vault_pod_status
      ignore_errors: true      

    - name: Install Vault using Helm (with dev mode and HA disabled, NodePort service)
      command: >
        helm install vault hashicorp/vault --namespace vault \
        --set server.ha.enabled=false \
        --set server.dev.enabled=true \
        --set server.dataStorage.enabled=false \
        --set server.service.type=NodePort \
        --set server.service.nodePort=31000 \
        --set ingress.enabled=false \
        --set server.dev.rootToken=root
      when: vault_pod_status.rc != 0          

    - name: Wait for pod to be running
      shell: |
        kubectl --context kind-{{ inventory_hostname }} get pods vault-0 -n vault -o jsonpath='{.status.phase}'
      register: pod_status
      until: pod_status.stdout == "Running"
      retries: 30  # Maximum retries
      delay: 5  # Delay between retries (in seconds)
      failed_when: pod_status.stdout != "Running"  # Fail if the pod is not running after retries

    - name: Print Vault Pod status
      debug:
        var: pod_status.stdout 

    - name: Enable PKI secrets engine
      command: kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault secrets enable pki
      ignore_errors: true   

    - name: Configure PKI secrets engine with CA certificate
      shell: |
        kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault write pki/root/generate/internal \
          common_name="mydomain.com" \
          ttl="8760h"

    - name: Configure PKI secrets engine to issue certificates
      shell: |
        kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault write pki/config/urls \ 
        issuing_certificates="http://vault.vault.svc.cluster.local:8200/v1/pki/ca" \
        crl_distribution_points="http://vault.vault.svc.cluster.local:8200/v1/pki/crl" 

    - name: Configure a role for certificate issuance
      shell: |
        kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault write pki/roles/istio-ca-{{ inventory_hostname }} \
          allowed_domains=istio-ca \
          allow_any_name=true  \
          enforce_hostnames=false \
          require_cn=false \
          allowed_uri_sans="spiffe://*" \
          max_ttl=72h

    - name: Create istio system ns
      command: >
        kubectl --context kind-{{ inventory_hostname }} apply -f ./istio/istio-ns.yaml 
      ignore_errors: true

    - name: Create istio cert issuer resources for vault access
      command: >
        kubectl --context kind-{{ inventory_hostname }} apply -f ./istio/issuer-sa.yaml \
        -f ./istio/issuer-sa-token.yaml \
        -f ./istio/issuer-crb.yaml
      ignore_errors: true            
          
    - name: Enable Kubernetes auth method in Vault
      shell: kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault auth enable --path={{ inventory_hostname }} kubernetes 
      ignore_errors: true      

    # - name: Get Kubernetes API server host from the cluster
    #   shell: |
    #     kubectl --context kind-{{ inventory_hostname }} config view --raw --minify --flatten -o jsonpath='{.clusters[0].cluster.server}'
    #   register: k8s_host 

    - name: Get Kubernetes CA certificate from the cluster
      shell: |
        kubectl --context kind-{{ inventory_hostname }} config view --raw --minify --flatten -o jsonpath='{.clusters[0].cluster.certificate-authority-data}' | base64 --decode
      register: k8s_ca      

    - name: Get Service Account token for istio cert issuer
      shell: |
        kubectl --context kind-{{ inventory_hostname }} get secret issuer-token-lmzpj -n istio-system -o jsonpath='{.data.token}' | base64 --decode
      register: sa_token                           

    - name: Configure Kubernetes auth method for east cluster
      shell: |
        kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault write auth/{{ inventory_hostname }}/config \
          token_reviewer_jwt="{{ sa_token.stdout }}" \
          kubernetes_host="https://kubernetes.default.svc" \
          kubernetes_ca_cert="{{ k8s_ca.stdout }}" \
          issuer="https://kubernetes.default.svc.cluster.local"
      when: inventory_hostname == 'east'           
          
    - name: Create pki policy
      shell: |
        kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault policy write pki - <<EOF
        path "pki*" {
          capabilities = ["create", "read", "update", "delete", "list"]
        }
        EOF
      ignore_errors: true        

    - name: Create Vault role for istio
      shell: |
        kubectl --context kind-{{ inventory_hostname }} exec -n vault -it vault-0 -- vault write auth/{{ inventory_hostname }}/role/issuer \
          bound_service_account_names=issuer \
          bound_service_account_namespaces=istio-system \
          policies=pki
          ttl=1h    

    - name: Print sa_token
      debug:
        var: sa_token.stdout  

    - name: Print k8s_host
      debug:
        var: k8s_host.stdout 

    - name: Print k8s_ca
      debug:
        var: k8s_ca.stdout  

    - name: Check if cert-manager is already installed
      command: kubectl --context kind-{{ inventory_hostname }} get deployment cert-manager -n cert-manager
      register: cert_manager_check
      ignore_errors: true

    - name: Install cert-manager using Helm if not installed
      command: >
        helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace 
        --set crds.enabled=true \
        --set global.logLevel=6 \   
        --version v1.16.0 \     
        --wait \
      when: cert_manager_check.failed  

    - name: Check if the Issuer resource exists in the istio-system namespace
      command: kubectl get issuer istio-ca -n istio-system
      register: issuer_check
      ignore_errors: true

    - name: Deploy vault issuer
      command: >
        kubectl --context kind-{{ inventory_hostname }} apply -f ./{{ inventory_hostname }}/vault-issuer.yaml  
      when: issuer_check.rc != 0   
      failed_when: issuer_apply.rc != 0
      changed_when: issuer_apply.rc == 0  

    - name: Inform if the Issuer resource already exists
      debug:
        msg: "Issuer resource 'istio-ca' already exists in the 'istio-system' namespace."
      when: issuer_check.rc == 0               

    - name: Install cert-manager-istio-csr using Helm
      command: >
        helm install cert-manager jetstack/cert-manager-istio-csr --namespace istio-system   
        --version v0.12.0 \     
        --wait \         

    # - name: Retrieve the initial admin password
    #   command: >
    #     kubectl --context kind-{{ inventory_hostname }} -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d 

